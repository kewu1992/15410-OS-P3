/**

@mainpage 15-410 Project 4

@author Ke Wu (kewu)
@author Jian Wang (jianwan3)

The following is an overview of the AMP design in P4.

1. Message passing interface
Every worker core has two queues: a send queue and a recv queue. Each queue will
be accessed by at most two cores: the worker core that owns the queue and the 
manager core that polls the queue. Each thread has a message associated with it.
Each time a syscall of an interprocessor request issues, the thread that invokes
the system call constructs its message with the following format, puts 
its message in the core's send queue, at the same time blocks itself (will not 
be scheduled by scheduler), and the scheduler on the local core chooses the next
thread to run. Each time during timer context switch, every scheduler will first
check if there's message in the core's recv queue, if so, the thread that's 
associated with the message will be resumed by scheduler and starts running 
from where it previously blocked when sending the interprocessor syscall and 
can continue to finish the syscall like examining the result stored in the 
message which has been modified by the thread that performs the interprocessor 
syscall on a different core. Finally the thread returns the result to user.

Form the view of the thread that sends the interprocessor syscall, the syscall
is very similar to a IPC/RPC because it is "synchronous". The threads is blocked
after sending its message and will be resumed by scheduler after the message is
sent back. It can also examine the result stored in the message. Notice that the
same message that's associated with the thread acts as both a request and a 
response for the interprocessor syscall. 

typedef struct {
    /** @brief A node to enable this message be put in a queue somewhere */
    simple_node_t node; // 12 bytes
    /** @brief The tcb of the thread that issues a interprocessor syscall */
    void* req_thr;      // 4 bytes
    /** @brief The index of the core where the requesting thread resides */
    int req_cpu;        // 4 bytes
    /** @brief Type of the message: can be request or response type */
    msg_type_t type;    // 4 bytes
    /** @brief Data field of the message, can be request or response data 
     *  Use union to limit message size. Data types like msg_data_fork_t
     *  contains data that's needed by fork(), etc.
     */
    union {
        msg_data_fork_t fork_data;
        msg_data_wait_t wait_data;
        ...
    } data; // maximum is 16 bytes
} msg_t;

/** @brief Interface on worker core side */
void worker_send_msg(msg_t* msg);
msg_t* worker_recv_msg();

/** @brief Interface on manager core side */
void manager_send_msg(msg_t* msg, int dest_cpu);
msg_t* manager_recv_msg();

The queue used in our design is the simple queue as we implemented in P3. It is
a simple double-ended FIFO queue without using malloc(). To avoid using 
malloc(), it is caller's responsibility to provide space for node of simple 
queue. The simple queue is NOT thread safe.

Besides, a message and a thread are highly related and are key to our design.
Each thread has a field in its tcb that points to a message owned exclusively
by the thread. And each message has a filed that points to its unique 
owner thread. The space of message is allocated when creating a thread. So send/
recv a message will not fail due to no resource problem. Another important idea
in our design is that send/recv a message is highly related to block/resume a 
thread. When a thread sends its message as a request of a interprocessor 
syscall, the thread blocks itslef. When the message is sent back as a response 
of the interprocessor syscall, the thread is resumed by scheduler. 

The relationship between send/recv message and block/resume thread can also be
found on the manager core. When the manager core receives a message, it will 
assign the message to a worker thread. The worker thread will perform the
interprocessor syscall based the syscall information stored in the message. 
When a worker thread serves for a message, we can regard the worker thread as
an "agent" that acts on behalf of the thread associated with the message. It's
like the thread is "resumed" and starts "running" on the manager core. On the 
other hand, after a worker thread finishes its work, stores result in the 
message and puts the message (as a response) to the recv queue of the core that 
issues the interprocessor syscall, it's like the thread associated with the 
message is "blocked" by the manager core (which will later be resumed on its 
local core).  This relationship is very important when we implements some 
system calls that might need to "block" a thread on the manager core. We don't
need to (and shouldn't) really block the worker thread. When a workter thread 
needs to "block" the thread it serves for on the manager core, the worker thread
just puts the message to a queue on the manager core (e.g. wait queue for wait 
syscall), and then it can start serving the next thread. When the event that
the message is waiting for is satisfied, the worker thread just needs to remove 
the message from the wait queue, and send the message back to the recv queue of
its local core. This is like the worker thread make the waiting thread 
"runnable".

This idea is very similar to what we have implemented in P3. In P3, we have 
discussed that changing the state of a thread such as running, runnable and 
block is just moving the thread between different queues. This argument is 
also true in P4 except that there are more queues (recv/send queues of local 
cores, queues of manager core) than P3.


2. Spinlock design

We have implemented a simple spinlock without supporting bounded waiting in P3.
Please refer to README of P3 for details of implementation, the time to use it,
etc. The main idea is that spinlock should only be used to protect a few lines
of code and be locked for a short time. 

In P4, we upgrade our spinlock to support bounded waiting. In P4, the only 
shared data structure that will be accessed by multiple cores is the message
queues. As indicated by the send and recv queue design above, there're at most 
two cores that will access a queue. So, the potential number of requesters is 
known. We can use the array method introduced in lecture "Synchronization #2" to
ensuring Bounded Waiting. 

/**@ brief spinlock type */
typedef struct {
    /** @brief Flag indicating if the lock is available */
    int available;
    /** @brief Flag indicating waiting status */
    int waiting[2];
} spinlock_t;

Notice that there are many places in our kernel that use spinlock. Most of
them (i.e. codes of P3) are actually only need disable/enable interrupt to
support exclusive access. This is becuase the design of P4 makes multiple cores
not access the same resources directly. These resources either have one 
copy for each core or will only be accessed by manager (e.g. singleton 
resource). The only resource that will be access by multiple cores are the 
message queues.

3. Overview of complicated system calls

3.1 fork():
Fork is the only syscall that lets the worker core that actually does the fork
sets its %cr3 to the %cr3 of the thread that needs to be forked while cloning 
page directory, which is inevitable, because the entire page direcotry and 
frames needs to be traversed. 

fork() is done in the following steps:

0) A thread issues a fork syscall 
1) the old thread on the old core does a thread_fork() and creates a new thread 
   (kernel stack) on the old core using the heap sapce of the old core.
2) the old thread constructs its message as FORK type, stores the tcb of the 
   new thread in the message and sends the message to manager core. Thus the old
   thread is blocked.
3) the manager core selects a worker core (refered as the new core) in round 
   robin order and forwards the old message (associated with the old thread) to
   the new core.
4) the new core's scheduler receives the message, and adds the *new thread* 
   (not the old thread) that stores in the message to the queue of scheduler.
5) new thread starts running on the new core, it performs the rest of fork(),
   including cloning page tables, creating pcb data structure, cloning swexn 
   handler, etc.
6) new thread constructs its message (refered as the new message) as 
   FORK_RESPONSE type, stores the result of fork() in the message, and sends the
   message to manager core. Thus the new thread is blocked.
7) the manager core receives the messgae. If the fork() succeeds, the manager 
   will modify the old message's (associated with the old thread) type as 
   FORK_RESPONSE, copy result from the new message to the old message and send
   the old message back to the old core. In the mean time, the manager core also
   sends the new message back to the new core. 
8) the old core receives the old message which contains the result of fork().
   The old thread starts running. The new core receives the new message, the
   new thread starts running.
9) If fork() fails in step 7, then the manager core will choose another core and
   forward the old message to the core. The process jumps to step 4.

The key point in the fork() is that the new thread is created on
the old core and then be forwarded to the new core to run. The new thread is
responsible for cloning page tables, creating pcb for itself on the new core,
and then sending fork response back. So the space of the kernel stack of the 
new thread belongs to the old core, and the space of page tables and pcb of 
the new thread belong to the new core.



3.2 vanish():
If the thread that invokes vanish() is not the last thraed of the task, then
only the resources of the thread need to be released. As implemented in P3, we
put the thread in a zombie list and let other threads free its resources during
context switch. The only difference is that in P4 every core has its own zombie
list to store its own zombie threads. A thread should be put to the zombie list
of the core where it was created on. For the thread that is created by 
thread_fork(), it is just the core that it is running on. But for the thread 
that is created by fork(), the core that it is running on is not the core that
is is created on. Recall that when fork(), the new thread is craeted on the old
core and then be forwarded to the new core. So this kind of thread should be put
back to the core that created it. This is done by sending a message VANISH_BACK,
then the manager core will forward this message to the original core. Thus the 
thread is temporaryly running on the original core, and put itself to the 
zomebie list of the original core.

If the thread is the last thread of the task, then the resources of the task
need to be released. A task has two data structures, pcb and pcb_vanish_wait_t.
pcb is allocated on the core that creates it which is easy to released. 
pcb_vanish_wait_t is allocated on the manager core, and needs to be freed on
manager core. As the name suggests, pcb_vanish_wait_t contains data that is
associated with vanish and wait, including wait_queue, child_exit_status_list,
etc. These data reside on the manager core so that the data of different tasks
can be accessed at the same time. For example, a child task may need to put its 
exit status to its father's child_exit_status_list and wake up a waiting thread
in the wait_queue of the partent task. However, child task and partent task may
belong to different core. Putting their pcb_vanish_wait_t data on the manager
core could reduce memory copy.

vanish() is done in the following steps:
0) A thread issues a vanish syscall 
1) If the thread is the last thread of the task, construct its message as 
VANISH, send its message to manager core. Operations such as sending exit
status to its parent(or init task), exporting unreapted child task to init task
will be done on the manager core. After these operations, the manager core frees
pcb_vanish_wait_t, and sends the message back. The thread frees pcb on the local
core. 
2) The thread constucts its message as VANISH_BACK, sends message to the manager
core. The manager core will send the message to the original core that creates
this thread.
3) The thraed puts itself to the zombie list of the original core.
4) Some other thread will free the resources of the thread during context 
   switch.



3.3 wait():
Most operations of wait() are completed on the manager core. As discussed 
before, a thread "block" on the manager core is just putting the message 
associated with the thread in a queue. And "make runnable" a thread is removing 
the message from the queue and sending the message back to the recv queue of the
local core. 

wait() is done typically in the following steps:
0) A thread issues a wait syscall
1) The thread constructs its message as WAIT, and sends the message to the 
   manager core. So the thread is blocked.
2) The manager core receives the message, checks its pcb_vanish_wait_t. If there
   is any available exit status in the child_exit_status_list, the worker thread
   just stores the status in the message, and sends the message back.
3) If there is no available exit status but the task has alive child task, the
   worker thread will "block" the thread on the manager core. This is done by
   putting the message of the thread in the wait_queue of its pcb_vanish_wait_t.
   Then the work thread is free to server the next message.
4) When a child task vanishes, it will send message to the manager core. When a
   worker thread tries to put the exit status of the child task to the partent
   task's child_exit_status_list, it will find the message that is "blocked"
   on the wait_queue of the partent task. The worker thread will remove the 
   message from the queue, store the exit status in the message, and send the
   message back to the recv queue of the local core. Thus the "blocking" thread
   becomes "runnable".
5) The local core receives the thread, the thread starts running again. It will
   examine the result stored in its message and return the result to user.



3.4 yield() and make_runnable():
These two system calls are special because they need to check resources of 
every core. For yield(), it needs to check the runnable queue of every 
scheduler. For make_runnable(), it needs to check the deschedule queue of every
core. We implemented this by making the thread that invokes the syscalls to 
"visit" (run on) every core one by one and check the queues of the core that it
running on. And finally, after the thread has visited all cores, it will go 
back to its own core and return the result to user.

For example, make_runnable() is done in the following steps:
0) A thread issues a make_runnable syscall
1) The thread constructs it message as MAKE_RUNNABLE, set the value of the 
   next_core field as the core it is running on (the original core). Thread 
   sends the message to the manager core, the thread is blocked (from the view 
   of the original core).
2) The manager core receives the message, calculates "the next core" as 
   next_core = (next_core + 1) % num_worker_cores, then sends the message to the
   recv queue of "the next core".
3) "The next core" receives the message, adds the thread to its scheduler's 
   queue. Form the view of "the next core", it makes the thraed runnable.
4) The thread starts running, it will check the deschedule queue of "the next 
   core". If it finds the thread that has the tid that make_runnable syscall 
   specifies, it makes the thread runnable and change the result field in
   its message.
5) The thread sends its message to the manager core. From the view of "the next
   core", the thread is blocked.
6) The manager core receives the message, it checks the result field to see 
   whether the syscall has succeeded. If so, the manager core will send the 
   message back to the recv queue of the original core immediately. Otherwise, 
   it will jump to step 2.
7) Finally, the thread will go back to the original core. It will return the
   result stored in its message back to user.

One issue is that when the thread switches to the core it will be on, it will
set its pcb to the pcb of the idle task of the core it will be running on. The
reason is that in our implementation each core has its own page tables that 
mapping virtual memory of kernel space to the physical frames. In other words,
all tasks running on the same core share the same page tables that mapping 
kernel space to frames. Tasks running on the difference cores have different 
kernel-space mapping page table. (However, the physical frames that they mapped
to are the same and shared by all cores). So when a thread switching cores, it 
sets its pcb to the idle task of the core it will be running on to avoid the 
large memory transferring of the page tabels. 

However, we realize that there is still some non-trivial memory transferring if
the thread switches the core temporarily. For example, the running kernel stack
space of the thread. However, it should be not very large (probably 100 bytes 
for a frame of the stack). Besides, we think the temporary core-switching is 
inevitable for yield() and make_runnable(). Some threads must check the queues
of every core. And our design tries to avoid "letting other threads do things
for you" because it will be painful, compltex and error prone (like vanish()).
So we try our best to let thread do its own work, like letting the new thread 
clone its page directory and page tables during fork() and checking queues of
cores during yield() and make_runnable().  



3.5 print():
Since console is a singleton resource, all print() syscalls need to be 
completed on the manager core. In order to let the manager core access the 
user buffer to be printed, a worker core mallocs a kernel buffer, copies the 
user buffer to the kernel buffer, sends the kernel buffer pointer to the
manager core, and the manager core accesses the kernel buffer through the
pointer. This is to avoid having the manager thread set %cr3 to be the 
thread that requests the print, which clears TLB on manager core; and, to avoid
changing the page table on manager core to access user buffer, which may crash
with other thread requesting accessing user space memory at the same time. As
kernel memory are directly mapped and accessable on every core, referencing
a kernel space buffer causes the minimal cost except some cache line miss that
is inevitable.

4. Weakness and improvement
Due to limited time and final week, we adhere to a solid working single-threaded
manager core that polls messages on worker cores and executes the system calls
itself, instead of a multi-threaded manager core. However, we would like to 
state our design here.

A multi-threaded manager core includes a mailbox thread and many worker threads.
The mailbox thread will poll the send queues of worker cores periodically and 
assign work(message) to worker threads. The worker threads are organized as a
thread pool, any work can be assigned to any thread as long as it is idle. 

Normally, a worker thread will not be blocked when it is working (except for 
the case that it is blocked due to mutex which ensures bounded waiting). If the
work can not be done immediately and needs to block (e.g. wait() and readline())
, the worker thread will just put the message in a wait queue like we discussed
previously and then the worker thread thinks it finishes its work and is ready 
for the next work. When there is no work need to be done (no message in send 
queues), a worker thread will be blocked in a queue. Later when some messages 
come, the mailbox thread will wake up the blocked thread and assign work to 
it.

The mailbox thread will be scheduled by scheduler only if it can make progress.
So if either there is no available message or there is no idle worker thread, 
the mailbox thread will be blocked. When a worker thread finishes its work, it 
will try to wake up the mailbox thread before the worker thread blocks itself. 
The interesting part is how to wake up the mailbox thread when a new message
comes. We find the previous slides in 410kern/smp/L36_P4.pdf which presents
a new kind of interrupt: Inter-Processor Interrupts (IPIs). We think this will 
be helpful for this problem. When a worker core sends a message to the manager
core, it also sends a inter-processor interrupt. This provides an opportunity 
for the interrupt handler of the manager core to wake up the mailbox thread.


*/
