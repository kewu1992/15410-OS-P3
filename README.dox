/**

@mainpage 15-410 Project 3

@author Ke Wu (kewu)
@author Jian Wang (jianwan3)


1. Lock
We implemented two locks for synchronization in kernel. One is spinlock, another
is mutex.

Spinlock is implemented using disable_interrupts() and xchg instruction. It can 
be used in either uni-processor or multi-processor environment. 
disable_interrupts() guarantees that no interrupt from the same processor will 
happen when spinlock is locked. Besides, xchg instruction and while loop are 
used to prevent threads of other processors from accessing the critical section. 
Because interrupts are disabled and while loop is used, spinlock should not
be locked in a very long time. Normally, spinlock is supposed to protect a few 
lines of code (e.g. the implementation of mutex) or very tricky data structure 
(e.g. queue of scheduler during context switch). Note that in interrupt handler,
we can only use spinlock. Otherwise a thread may be blocked before interrupt is
acked!

Mutex is built on the top of spinlock. It doesn't rely on disable_interrupts()
to make sure exclusive access. If a thread can not get the mutex immediately 
when it try to lock the mutex, it will be blocked on the mutex and stored in the
queue of mutex. It will not be scheduled by scheduler until the mutex holder 
give the lock to the blocked thread and make it runnable. Because we use mutex
to protect malloc(), we can not rely on malloc() to construct node for queue 
when a thread will enqueue. Instead, the stack space of blocked threads is used 
to construct node for queue. Because the stack of the procedure call will not 
be destroied until the blocked thread get the mutex, so it is safe.

2. Running thread, runnable thread and blocked thread
A thread is either running, runnable or blocked on some objects. A running 
thread is the thread that currently has the processor. A runnable thread is 
a thread that stored in the queue of scheduler. A blocked thread is a thread
that stored in the queue of the object that it is blocked on. Possible objects
to block a thread includng mutex, sleep(), deschedule(), wait(), vanish(), etc. 
Each object has its own queue. 

Note that runninng, runnable and blocked are not the same as states of a thread
defined in tcb. This is just a high level description. 

All activities of threads can be described as "moving threads between queues"
- Context switch means put the thread before context switch to the queue of 
scheduler and remove the thread after context switch from the queue of scheduler
- Block a thread is just put a thread to the queue of the object it blocked on.
- Make runnable a thread is remove it from the queue of the object it blocked on 
and put it to the queue of scheduler.
- Resume (wake up) a thread is remove it from the queue of the object it blocked 
on and put the thread that execuing resume to the queue of scheduler. Note that
in this way, we can always switches to the waiting thread instead of resuming 
the interrupted thread when the waiting event completes.

Based on the description discussed above, many system calls such as sleep(), 
deschedule(), make_runnable(), wait(), readline() can be implemented easily. 

3. Preemption of kernel
Except for some parts of kernel that are protected by spinlock, most parts of 
our kernel can be interrupted and context switch to another thread. To do so, 
we stored/resotred not only generic registers and EFLAGS, but also value of 
k_esp (the value of esp when the last context switch happens, this is useful if
context switch is happened during a context switch) and value of %cr2 (this is
useful if a context switch is happened during an exception)

4. Fork and thread_fork
These two system calls are very similar. Thread_fork is clone the kernel stack
of the original thread, fork is thread_fork plus clone page table (roughly..).
The key question is how to make sure the kernel stack of the original thread is 
unchanged during clone? We solve this problem by cloning kernel stack of 
original thread during context switch. Because when context switch, the 
kernel stack of the original thread is 'sealed' and will be unchanged until
the next switch back to the original thread. It is a good time to do cloning. 
In a addition, beacuse all kernel stack is cloned for the new thread, when it 
returns in the next time context switch it should behave exactly the same as 
the original thread. 

Note that although fork and thread_fork happens during context switch. They can
still be interrupted by interrupts and preempted by other threads. 


5. Vanish
Exit status reporting for vanishing task
When the thread to vanish is the last one in a task, it needs to report its
exit status to its parent task. The thread will first check if its parent 
task is alive, which is accomplished by consulting a hashtable that maps a pid
to a pcb. If pcb corresponding to the pid exists in the hashtable, then the 
parent task is alive and the vanisning task can put its exit status to parent 
task's child_exit_status_list; else, the vanishing task will put its exit 
status to the init task's child_exit_status_list. There's a race condition 
between the last thread checking parent task's aliveness and parent task's
vanishing itself, if timing is proper, a child task's exit status can not be
collected. Fortunately, the problem is solved by properly acquiring and 
releasing locks: when a task vanishes, it first grabs the hashtable's lock,
then the parent task's pcb lock , then release the hashtable lock so that 
other tasks can use it, then add its exit status to the child_exit_status_list
in the parent's pcb, and finally release parent task's pcb lock. On the other
hand, a vanishing task to delete its entry in the hashtable to mark its death
so that its children will report exit status to init task in the future will
try to get the hashtable lock first, then get its own pcb lock, delete the
entry in the hashtable, then release its pcb lock and hash table lock. The
effect of this approach is that, 1. a task can't delete its entry in the
hashtable while its children is reporting exit status to it (because its 
chilren is holding its pcb lock and it can't delete its entry in hashtable
without its own pcb lock). 2. a vanishing task who successfully put its 
exit status to its parent's child_exit_status_list is guaranteed to be
noticed by parent task because parent.

Resource freeing for vanishing task
A vanishing thread has tcb and kernel stack to free, and if it's the
last thread in a task, it also needs to free resources in pcb like page 
directory and pcb itself. The approach we use is that a thread frees as much 
as it can itself. That said, when a thread vanishes and is the last thread
in a task, it first sets pcb to that of the init task (because it won't go 
back to user space and kernel memory is shared across tasks so there's no side 
effect) and safely frees its own pcb and address space. However, since a 
vanishing thread cannot free its own kernel stack space (because it's using) 
and its tcb (because timer interrupt may happen during vanishing and tcb is 
needed during context switch), so that it puts itself in the system-wide zombie
list, and the next thread to run can check the zombie list before it returns
to previous execution and reap it. There're some subtleties for this approach. 
1. After a vanishing thread puts itself to the system-wide thread zombie list 
and proceeds to blocking but hasn't fully blocked, timer interrupt may happen 
and the the next thread starts to run and removes one zombie thread that happens
to be not the vanisning thread we are talking about, then there's a chance that
the not fully blocked vanisning thread starts to run again and is likely to
reap itself when checking the zombie list. 2 The other issue is that even if
reaping thread isn't the thread to reap, because freeing resouces needs the 
malloc library lock, it's entirely possible that the reaping thread may be in
some critical section with the malloc library lock in its previous term, and
in this case, it should not proceed to free but also it's hard to revert
in deep function stacks. To solve these 2 issues, we devised a "try lock" 
scheme that only after a thread gets all the locks it needs and it's not the
thread to reap does it proceeds to reaping.


6. Memory Management
The virtual memory system uses paging and ZFOD (Zero-fill on demand) with a 
physical frame allocator that supports allocating and freeing single page size 
frames.

Use of page table entry's "For programmer's use" bits
There are 3 bits in page table entry that are "For programmer's use", so that
each of them can be of use for a purpose. Specifically, we use them to mark a 
page as ZFOD, start page of new_pages syscall, and end page of new_pages 
syscall, repectively. The advantage of dedicating a bit to ZFOD is obvious.
Say, when we serve a new_pages syscall call, we marked the page table entries 
of pages to allocate as ZFOD, so that in the future, when a page fault occurs,
we can check the corresponding bit to see if the fault is caused by ZFOD, and
if so, we can allocate a new frame and clear the bit. The reason for the other 
two bits to be used to mark the start and end page of a new_pages syscall is 
to save the effort of recording regions that are results of new_pages syscall.
With the help of the two bits, the check of validness of free_page's base 
parameter is trivial and we can quickly find the end page of a previous 
new_pages syscall as we traverse and free. The extra storage overhead for
new_pages and remove_pages are thus near zero.

Preemptibility in memory
Each task has its own unique page directory, so that page directories in 
different tasks can be accessed concurrently by different tasks without 
interfering with each other. However, within a task, different threads share
page tables in a page directory and there're race conditions when accessing
this shared data structure concurrently. Thus, locks are needed for page table
consistency within a task. We devised a solution that every 16 consecutive
page tables share a mutex lock to achieve concurrency with consistency 
protection. This means there're 64 locks for each page directory, which is not 
a too big memory overhead. Each time a thread needs to perform some operation
in a memory region, it needs to acquire all locks for the page tables that 
cover the region before it can proceed. Usually locks are hold till the
operation is complete as it's not tolerable for a new_pages syscall to acquire
locks, check if certain region has some pages already allocated and release 
the lock, do some calculation, acquire locks again, and do the allocation,
because it's entirely possible pages are removed by other threads during the 
time the thread does the calculation.

Physicall memory allocator uses a segment tree implementation instead of a 
naive bitmap to improve performance. Although only a single mutex is used to 
protect the physical memory allocation, this is not a real preemptibility 
bottle neck in the sense that most of the memory operations like new_pages,
clone page table, does not require frames at the time of the operation since
ZFOD is used and a large portion of the pages pointed to the system-wide 
all-zero frame. Besides, segment tree's O(log(n)) allocation and deallocation 
time complexity makes each thread's time in critical section tolerable when a 
frame is really needed.

7 Miscellaneous
Atomic counter
There are many critical sections that involve only incrementing or 
decrementing a counter, to avoid using too much spinlock (which disables 
interrupt temporarily) and mutex (which may involve context switch to previous
lock holder), we devised an approach that uses the cmpxchg (CAS) machine 
instruction to implement atomic arithmetics.

In our design, we tried to avoid using malloc to get space from heap as less
often as possible. 

Tcb locating
We have divided the entire kernel space into an imagined array, with each 
element of size K_STACK_SIZE (For 16 MB kernel space and K_STACK_SIZE = 8192, 
the array has 2048 elements). When allocating a new thread, kernel stack is 
allocated by calling smemalign(K_STACK_SIZE, K_STACK_SIZE) so that the %esp of
a thread that's executing normally in kernel mode is guaranteed to fall within 
a slot in the array. With the thread's %esp, we can get the tcb of the thread
in constant time by looking up the table array.

*/

