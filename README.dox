/**

@mainpage 15-410 Project 3

@author Ke Wu (kewu)
@author Jian Wang (jianwan3)


1. Lock
We implemented two locks for synchronization in kernel. One is spinlock, another
is mutex.

Spinlock is implemented using disable_interrupts() and xchg instruction. It can 
be used in either uni-processor or multi-processor environment. 
disable_interrupts() guarantees that no interrupt from the same processor will 
happen when spinlock is locked. Besides, xchg instruction and while loop are 
used to prevent threads of other processors from accessing the critical section. 
Because interrupts are disabled and while loop is used, spinlock should not
be locked in a very long time. Normally, spinlock is supposed to protect a few 
lines of code (e.g. the implementation of mutex) or very tricky data structure 
(e.g. queue of scheduler during context switch).

Mutex is built on the top of spinlock. It doesn't rely on disable_interrupts()
to provide exclusive access. If a thread can not get the mutex immediately 
when it try to lock the mutex, it will be blocked on the mutex and stored in the
queue of mutex. It will not be scheduled by scheduler until the mutex holder 
give the lock to the blocked thread and make it runnable. Because we use mutex
to protect malloc(), we can not rely on malloc() to construct node for queue 
when a thread will enqueue. Instead, the stack space of blocked threads is used 
to construct node for queue. Because the stack of the procedure call will not 
be destroied until the blocked thread get the mutex, it is safe.

Protection scope of spinlock and mutex. You can invoke spinlock_lock() inside a 
critical section that is protected by mutex to ensure no interrupt will happen 
in a smaller but more importent critical section. But you can't invoke 
mutex_lock() in a critical section that is protected by spinlock. Becasue
mutex_lock() will invoke enable_interrupts() which will 'unlock' the 
spinlock. Two common mistakes are:  1) call malloc() in scheduler. malloc() is
protected by mutex while scheduler queue is protcted by spinlock.  2) using 
mutex in interrupt handler which is using interrupt gate. 


2. Preemption of kernel
Except for some parts of kernel that are protected by spinlock, most parts of 
our kernel can be interrupted and context switch to another thread. To do so, 
we stored/resotred not only generic registers and EFLAGS, but also value of 
k_esp (the value of esp when the last context switch happens, this is useful if
context switch is happened during a context switch) and value of %cr2 (this is
useful if a context switch is happened during an exception).

The code that are protected by spinlock including 1) scheduler's queue. It is 
impossible to protect it with mutex. Because mutex may block the thread that
can not get the lock, which will result in a context switch and another 
operation of scheduler's queue;  2) implementation of mutex;  3) timer 
interupt handler and its related syscall sleep();  4) keyboard interrupt handler
and its realated functions readline() and putbyte().

There are two reasons that we can't use mutex to protect interrupt handlers 
and funcitons sharing data with interrupt handlers:
1) Interrupt gate must be used for interrupt handlers. For example, in keyboard
handler, if trap gate is used and a timer interrupt comes in while the 
keyboard interrupt handler is manipulating data of readline() in 
resume_reading_thr(). The timer interrupt causes a context switch and the next 
thread might be running in readline_syscall_handler() and about to manipulate 
data of readline(). In this case, the data of readline() might be in an 
inconsistent state and some bad things will happen.  
2) In the functions that sharing data with interrupt handlers 
(e.g. readline_syscall_handler()), mutex can not be used either. Because 
interrupt handlers are using interrupt gate which disables all interrupts. 
If mutex is used, when an interrupt comes in and it calls mutex_lock() to access
shared data, mutex_lock() will call enable_interrupts() and make interrupt gate
useless.


3. Running, runnable and blocked thread
A thread is either running, runnable or blocked on some objects. A running 
thread is the thread that currently has the processor. A runnable thread is 
a thread that stored in the queue of scheduler. A blocked thread is a thread
that stored in the queue of the object that it is blocked on. Possible objects
to block a thread includng mutex, sleep(), deschedule(), wait(), vanish(), etc. 
Each object has its own queue. 

Note that runninng, runnable and blocked are not the same as states of a thread
defined in tcb. This is just a high level description. 

All activities of threads can be described as "moving threads between queues"
- Context switch means put the thread before context switch to the queue of 
scheduler and remove the thread after context switch from the queue of scheduler
- Block a thread is just put a thread to the queue of the object it blocked on.
- Make runnable a thread is remove it from the queue of the object it blocked on 
and put it to the queue of scheduler.
- Resume (wake up) a waiting thread is remove it from the queue of the object it
blocked on and put the interrupted thread to the queue of scheduler. Note that
in this way, we can always switches to the waiting thread instead of resuming 
the interrupted thread when the waiting event completes.

Based on the description discussed above, many system calls such as sleep(), 
deschedule(), make_runnable(), wait(), readline() can be implemented easily. 



4. Fork and thread_fork
These two system calls are very similar. Thread_fork is clone the kernel stack
of the original thread, fork is thread_fork plus clone page table (roughly..).
The key question is how to make sure the kernel stack of the original thread is 
unchanged during clone? We solve this problem by cloning kernel stack of 
original thread during context switch. Because when context switch, the 
kernel stack of the original thread is 'sealed' and will be unchanged until
the next switch back to the original thread. It is a good time to do cloning. 
In a addition, beacuse the entire kernel stack is cloned for the new thread, 
when the new thread switches back in the next time it should behave 
exactly the same as the original thread. 

Note that although fork and thread_fork happens during context switch. They can
still be interrupted by interrupts and preempted by other threads. 


5. Vanish
Exit status reporting for vanishing task
When the thread to vanish is the last one in a task, it needs to report its
exit status to its parent task. The thread will first check if its parent 
task is alive, which is accomplished by consulting a hashtable that maps a pid
to a pcb. If pcb corresponding to the pid exists in the hashtable, then the 
parent task is alive and the vanisning task can put its exit status to parent 
task's child_exit_status_list; else, the vanishing task will put its exit 
status to the init task's child_exit_status_list. There's a race condition 
between the last thread checking parent task's aliveness and parent task's
vanishing itself, if timing is proper, a child task's exit status can not be
collected. Fortunately, the problem is solved by properly acquiring and 
releasing locks: when a task vanishes, it first grabs the hashtable's lock,
then the parent task's pcb lock , then release the hashtable lock so that 
other tasks can use it, then add its exit status to the child_exit_status_list
in the parent's pcb, and finally release parent task's pcb lock. On the other
hand, a vanishing task will delete its entry in the hashtable to mark its death
so that its children will report exit status to init task in the future. To do 
so, the vanishing task will try to get the hashtable lock first, then get 
its own pcb lock, delete the entry in the hashtable, then release its pcb lock 
and hash table lock. The effect of this approach is that, 1. a task can't delete
its entry in the hashtable while its children is reporting exit status to it 
(because its chilren is holding its pcb lock and it can't delete its entry 
in hashtable without its own pcb lock). 2. a vanishing task who successfully put
its exit status to its parent's child_exit_status_list is guaranteed to be
noticed by parent task because parent.

Resource freeing for vanishing task
A vanishing thread has tcb and kernel stack to free, and if it's the
last thread in a task, it also needs to free resources in pcb like page 
directory and pcb itself. The approach we use is that a thread frees as much 
as it can itself. That said, when a thread vanishes and is the last thread
in a task, it first sets pcb to that of the init task (because it won't go 
back to user space and kernel memory is shared across tasks so there's no side 
effect) and safely frees its own pcb and address space. However, since a 
vanishing thread cannot free its own kernel stack space (because it's using) 
and its tcb (because timer interrupt may happen during vanishing and tcb is 
needed during context switch), so that it puts itself in the system-wide zombie
list, and the next thread to run (during context switch) can check the zombie 
list before it returns to previous execution and reap it. There're some 
subtleties for this approach. 1. After a vanishing thread puts itself to the 
system-wide thread zombie list and proceeds to blocking but hasn't fully 
blocked, timer interrupt may happen and the the next thread starts to run and 
removes one zombie thread that happens to be not the vanisning thread we are 
talking about, then there's a chance that the not fully blocked vanisning 
thread starts to run again and is likely to reap itself when checking the zombie 
list. 2 The other issue is that even if reaping thread isn't the thread to reap,
because freeing resouces needs the malloc library lock, it's entirely possible 
that the reaping thread may be in some critical section with the malloc library 
lock in its previous term, and in this case, a self-deadlock might happen. 
To solve these 2 issues, we devised a "try lock" scheme that only after a 
thread gets all the locks it needs and it's not the thread to reap does it 
proceeds to reaping.


6. Memory Management
The virtual memory system uses paging and ZFOD (Zero-fill on demand) with a 
physical frame allocator that supports allocating and freeing single page size 
frames.

Use of page table entry's "For programmer's use" bits
There are 3 bits in page table entry that are "For programmer's use", so that
each of them can be of use for a purpose. Specifically, we use them to mark a 
page as ZFOD, start page of new_pages syscall, and end page of new_pages 
syscall, repectively. The advantage of dedicating a bit to ZFOD is obvious.
Say, when we serve a new_pages syscall call, we marked the page table entries 
of pages to allocate as ZFOD, so that in the future, when a page fault occurs,
we can check the corresponding bit to see if the fault is caused by ZFOD, and
if so, we can allocate a new frame and clear the bit. The reason for the other 
two bits to be used to mark the start and end page of a new_pages syscall is 
to save the effort of recording regions that are results of new_pages syscall.
With the help of the two bits, the check of validness of free_page's base 
parameter is trivial and we can quickly find the end page of a previous 
new_pages syscall as we traverse and free. The extra storage overhead for
new_pages and remove_pages are thus near zero.

Preemptibility in memory
Each task has its own unique page directory, so that page directories in 
different tasks can be accessed concurrently by different tasks without 
interfering with each other. However, within a task, different threads share
page tables in a page directory and there're race conditions when accessing
this shared data structure concurrently. Thus, locks are needed for page table
consistency within a task. We devised a solution that every 16 consecutive
page tables share a mutex lock to achieve concurrency with consistency 
protection. This means there're 64 locks for each page directory, which is not 
a too big memory overhead. Each time a thread needs to perform some operation
in a memory region, it needs to acquire all locks for the page tables that 
cover the region before it can proceed. Usually locks are hold till the
operation is complete as it's not tolerable for a new_pages syscall to acquire
locks, check if certain region has some pages already allocated and release 
the lock, do some calculation, acquire locks again, and do the allocation,
because it's entirely possible pages are removed by other threads during the 
time the thread does the calculation.

Physicall memory allocator uses a segment tree implementation instead of a 
naive bitmap to improve performance. Although only a single mutex is used to 
protect the physical memory allocation, this is not a real preemptibility 
bottle neck in the sense that most of the memory operations like new_pages,
clone page table, does not require frames at the time of the operation since
ZFOD is used and a large portion of the pages pointed to the system-wide 
all-zero frame. Besides, segment tree's O(log(n)) allocation and deallocation 
time complexity makes each thread's time in critical section tolerable when a 
frame is really needed.

7 Miscellaneous
Atomic counter
There are many critical sections that involve only incrementing or 
decrementing a counter, to avoid using too much spinlock (which disables 
interrupt temporarily) and mutex (which may involve context switch to previous
lock holder), we devised an approach that uses the cmpxchg (CAS) machine 
instruction to implement atomic arithmetics.

In our design, we tried to avoid using malloc to get space from heap as less
often as possible, especialy when a thread needs to block itself and needs to
construct a node to enqueue. It's because malloc() is not thread-safe and 
we use mutex to protect it. 
For example, in the queue of scheduler, we use spinlock to protect the queue. 
If a thread using malloc() to construct a node when it enqueues, it will 
enable_interrupts() when calling mutex_lock() which will unlock our spinlock 
that protects the queue of scheduler!
Another example is mutex itslef. Since we use mutex to protect malloc, we can 
not rely on malloc() to construct node for the queue of mutex.
The reason why the stack space is safe to be used for node of queue is that 
the thread will then be 'blocked' at the queue. So the stack of the procedure
call will not be destroied unitl the thread is unblocked and dequeued from the
queue. For constructing the node of scheduler queue, some memory spce is
reserved during context switch (please refer to asm_context_switch.S for 
details). 

Tcb locating
We have divided the entire kernel space into an imagined array, with each 
element of size K_STACK_SIZE (For 16 MB kernel space and K_STACK_SIZE = 8192, 
the array has 2048 elements). When allocating a new thread, kernel stack is 
allocated by calling smemalign(K_STACK_SIZE, K_STACK_SIZE) so that the %esp of
a thread that's executing normally in kernel mode is guaranteed to fall within 
a slot in the array. With the thread's %esp, we can get the tcb of the thread
in constant time by looking up the table array.

8. Known bugs
1) Kernel stack overflow handling:
We detect if the kernel stack of a thread is overflow in every timer interrupt.
But our kernel doesn't try to recovery from the error when a thread is 
stack overflow. It is not easy to do so. The kernel can not just kill
the thread immediately, because the thread may manipulating some 
kernel data structure when it is interrupted. So our kernel will just
panic() and let the developer to handle with this error. To be fair, 
in our implementation of the kernel, it is very unlikely that a thread
will stack overflow. When it happens, it may indicate some bugs.

2) O(n) time for yield(tid)
When a thread invokes the system call yield(tid) where tid is a specific thread, 
our context switcher needs to search the queue of scheduler to find if the 
thread is in the queue and remove it from the queue. We realize this is an O(n)
operation and it violates the requirement in the handout that any operation of 
scheduler should be done in constant time. But we don't have time to fix this 
issue. 

*/

